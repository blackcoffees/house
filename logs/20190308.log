2019-03-08 09:36:52,131 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:36:52,138 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:36:52,150 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:36:52,204 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:36:52,697 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:36:52,701 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:36:52,704 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:37:03,051 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.108.65:53281
2019-03-08 09:37:03,604 - _legacy.py[line:154] - CRITICAL: Unhandled error in Deferred:
2019-03-08 09:37:03,605 - _legacy.py[line:154] - CRITICAL: 
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\crawler.py", line 98, in crawl
    six.reraise(*exc_info)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\crawler.py", line 81, in crawl
    start_requests = iter(self.spider.start_requests())
  File "E:\workspace\pycharm\house_spider\scrapy_spider\spiders\house.py", line 38, in start_requests
    region = self.list_region[self.region_index]
IndexError: list index out of range
2019-03-08 09:37:13,875 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:37:13,881 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:37:13,891 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:37:13,940 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:37:14,378 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:37:14,384 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:37:14,388 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:37:42,867 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:139.129.207.72:808
2019-03-08 09:38:08,815 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:38:08,821 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:38:08,831 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:38:08,878 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:38:09,321 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:38:09,325 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:38:09,329 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:38:41,826 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.105.25:53281
2019-03-08 09:39:22,125 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:39:22,134 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:39:22,147 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:39:22,197 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:39:22,729 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:39:22,733 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:39:22,737 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:41:36,792 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:111.40.84.73:9797
2019-03-08 09:41:40,542 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:41:40,750 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:41:40,752 - middlewares.py[line:109] - INFO: Spider opened: real_estate
2019-03-08 09:42:40,770 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:43:03,493 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 09:43:03,497 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 23495,
 'downloader/request_count': 37,
 'downloader/request_method_count/GET': 37,
 'downloader/response_bytes': 1159648,
 'downloader/response_count': 37,
 'downloader/response_status_count/200': 36,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'cancelled',
 'finish_time': datetime.datetime(2019, 3, 8, 1, 43, 3, 497000),
 'item_scraped_count': 2166,
 'log_count/DEBUG': 2204,
 'log_count/INFO': 10,
 'request_depth_max': 36,
 'response_received_count': 37,
 'scheduler/dequeued': 36,
 'scheduler/dequeued/memory': 36,
 'scheduler/enqueued': 37,
 'scheduler/enqueued/memory': 37,
 'start_time': datetime.datetime(2019, 3, 8, 1, 41, 40, 752000)}
2019-03-08 09:43:03,497 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 09:45:39,104 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:45:39,111 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:45:39,575 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:45:39,631 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:45:40,105 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:45:40,111 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:45:40,114 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:48:13,624 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:139.129.207.72:808
2019-03-08 09:48:13,657 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:48:13,684 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:48:22,085 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:48:22,089 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:48:22,094 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:48:22,095 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:48:22,096 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:48:22,161 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-03-08 09:48:22,283 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:48:22,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:48:23,930 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 09:49:37,436 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.108.53:53281
2019-03-08 09:49:37,437 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:49:37,437 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:49:54,378 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.245.99.52:80
2019-03-08 09:50:04,552 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 09:50:04,559 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20231,
 'downloader/request_count': 36,
 'downloader/request_method_count/GET': 36,
 'downloader/response_bytes': 1149884,
 'downloader/response_count': 36,
 'downloader/response_status_count/200': 36,
 'finish_reason': 'cancelled',
 'finish_time': datetime.datetime(2019, 3, 8, 1, 50, 4, 557000),
 'item_scraped_count': 2146,
 'log_count/DEBUG': 2186,
 'log_count/ERROR': 1,
 'log_count/INFO': 20,
 'request_depth_max': 36,
 'response_received_count': 36,
 'scheduler/dequeued': 36,
 'scheduler/dequeued/memory': 36,
 'scheduler/enqueued': 37,
 'scheduler/enqueued/memory': 37,
 'start_time': datetime.datetime(2019, 3, 8, 1, 48, 13, 685000)}
2019-03-08 09:50:04,559 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 09:50:22,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:50:49,094 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:50:49,101 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:50:50,730 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:50:50,778 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:50:51,223 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:50:51,229 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:50:51,230 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:51:17,378 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:125.46.0.62:53281
2019-03-08 09:51:17,407 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:51:17,420 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:51:19,009 - httperror.py[line:55] - INFO: Ignoring response %(response)r: HTTP status code is not handled or not allowed
2019-03-08 09:51:19,015 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 09:51:19,016 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 370,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 1151,
 'downloader/response_count': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 1, 51, 19, 16000),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/404': 1,
 'log_count/DEBUG': 2,
 'log_count/INFO': 9,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 3, 8, 1, 51, 17, 421000)}
2019-03-08 09:51:19,017 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 09:51:19,029 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:51:19,032 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:51:19,036 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:51:19,036 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:51:19,038 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:51:42,496 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.85.196.75:9797
2019-03-08 09:51:42,618 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:51:42,619 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:52:56,016 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:52:56,022 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:52:57,576 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:52:57,634 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:52:58,117 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:52:58,122 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:52:58,125 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:54:52,414 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.108.73:53281
2019-03-08 09:54:52,453 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:54:52,484 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:55:52,484 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:56:52,484 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:56:58,749 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 09:56:58,752 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 5,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 6530,
 'downloader/request_count': 12,
 'downloader/request_method_count/GET': 12,
 'downloader/response_bytes': 253687,
 'downloader/response_count': 7,
 'downloader/response_status_count/200': 7,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 1, 56, 58, 750000),
 'item_scraped_count': 479,
 'log_count/DEBUG': 492,
 'log_count/ERROR': 1,
 'log_count/INFO': 10,
 'request_depth_max': 7,
 'response_received_count': 7,
 'retry/count': 4,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 2,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 12,
 'scheduler/dequeued/memory': 12,
 'scheduler/enqueued': 12,
 'scheduler/enqueued/memory': 12,
 'start_time': datetime.datetime(2019, 3, 8, 1, 54, 52, 486000)}
2019-03-08 09:56:58,753 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 09:57:09,742 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:57:09,743 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:57:12,191 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:57:12,196 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:57:12,200 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:57:12,200 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:57:12,203 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:57:14,099 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.27.24.114:82
2019-03-08 09:57:14,214 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:57:14,216 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:57:29,368 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 09:57:29,375 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 09:57:29,395 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:57:29,448 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:57:29,924 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:57:29,930 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:57:29,931 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:58:37,526 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.202.219.81:80
2019-03-08 09:58:37,558 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:58:37,572 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:58:37,585 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 09:58:37,588 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:58:37,592 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:58:37,594 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:58:37,595 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 09:58:38,882 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:122.114.71.79:808
2019-03-08 09:58:38,994 - engine.py[line:256] - INFO: Spider opened
2019-03-08 09:58:38,996 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 09:59:00,483 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:122.114.71.79:808
2019-03-08 10:02:23,650 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:02:23,651 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:02:23,749 - scraper.py[line:208] - ERROR: Error downloading %(request)s
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
ResponseFailed: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'non-integer status code', '<h1>\xd5\xca\xba\xc5\xc8\xcf\xd6\xa4\xca\xa7\xb0\xdc ...</h1>')>]
2019-03-08 10:02:23,849 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 10:02:23,851 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 831,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 2, 2, 23, 851000),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 11,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 3, 8, 1, 58, 38, 997000)}
2019-03-08 10:02:23,852 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 10:02:24,109 - scraper.py[line:208] - ERROR: Error downloading %(request)s
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
ResponseFailed: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'non-integer status code', '<h1>\xd5\xca\xba\xc5\xc8\xcf\xd6\xa4\xca\xa7\xb0\xdc ...</h1>')>]
2019-03-08 10:02:24,210 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 10:02:24,213 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 2050,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 643,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 2, 2, 24, 212000),
 'log_count/DEBUG': 9,
 'log_count/ERROR': 2,
 'log_count/INFO': 22,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 3, 8, 1, 58, 37, 573000)}
2019-03-08 10:02:24,213 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 10:07:41,884 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 10:07:41,892 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 10:07:41,901 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 10:07:41,960 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:07:42,430 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:07:42,436 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:07:42,437 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:07:47,331 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.196.204.61:80
2019-03-08 10:07:47,355 - engine.py[line:256] - INFO: Spider opened
2019-03-08 10:07:47,367 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:07:47,381 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 10:07:47,384 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:07:47,388 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:07:47,391 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:07:47,391 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:07:47,487 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.196.204.61:80
2019-03-08 10:07:47,584 - engine.py[line:256] - INFO: Spider opened
2019-03-08 10:07:47,585 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:07:47,898 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.196.204.61:80
2019-03-08 10:08:35,211 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:09:26,520 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:123.139.56.238:9999
2019-03-08 10:09:26,536 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:09:26,538 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:09:27,953 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:123.139.56.238:9999
2019-03-08 10:10:03,861 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:10:03,861 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:10:12,365 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:123.139.56.238:9999
2019-03-08 10:10:16,002 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:10:16,213 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:123.139.56.238:9999
2019-03-08 10:10:16,323 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:10:20,641 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:222.74.61.98:53281
2019-03-08 10:10:25,346 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:222.74.61.98:53281
2019-03-08 10:10:34,996 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:222.74.61.98:53281
2019-03-08 10:10:35,107 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:10:37,516 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:1.196.160.90:9999
2019-03-08 10:10:37,526 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:11:22,576 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:111.40.84.73:9999
2019-03-08 10:11:22,578 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:11:22,579 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:11:29,753 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 10:11:29,760 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 21965,
 'downloader/request_count': 39,
 'downloader/request_method_count/GET': 39,
 'downloader/response_bytes': 1017944,
 'downloader/response_count': 39,
 'downloader/response_status_count/200': 39,
 'finish_reason': 'cancelled',
 'finish_time': datetime.datetime(2019, 3, 8, 2, 11, 29, 760000),
 'item_scraped_count': 1849,
 'log_count/DEBUG': 2438,
 'log_count/ERROR': 5,
 'log_count/INFO': 32,
 'request_depth_max': 39,
 'response_received_count': 39,
 'scheduler/dequeued': 39,
 'scheduler/dequeued/memory': 39,
 'scheduler/enqueued': 40,
 'scheduler/enqueued/memory': 40,
 'start_time': datetime.datetime(2019, 3, 8, 2, 7, 47, 367000)}
2019-03-08 10:11:29,762 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 10:11:40,523 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:11:40,957 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:11:46,502 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:1.196.160.90:9999
2019-03-08 10:11:47,598 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:12:00,128 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:12:00,423 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:12:14,263 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:111.198.154.116:8888
2019-03-08 10:12:47,585 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:13:30,819 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:13:31,474 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:14:14,782 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:219.245.3.4:3128
2019-03-08 10:14:14,783 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:14:34,322 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:14:34,611 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-08 10:14:37,819 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:61.184.185.68:3128
2019-03-08 10:14:47,585 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:15:47,585 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:16:47,585 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:16:49,234 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 10:16:49,236 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 6,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 74979,
 'downloader/request_count': 176,
 'downloader/request_method_count/GET': 176,
 'downloader/response_bytes': 8592558,
 'downloader/response_count': 170,
 'downloader/response_status_count/200': 170,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 2, 16, 49, 236000),
 'item_scraped_count': 2738,
 'log_count/DEBUG': 4803,
 'log_count/ERROR': 14,
 'log_count/INFO': 37,
 'request_depth_max': 170,
 'response_received_count': 170,
 'retry/count': 5,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 4,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'scheduler/dequeued': 176,
 'scheduler/dequeued/memory': 176,
 'scheduler/enqueued': 176,
 'scheduler/enqueued/memory': 176,
 'start_time': datetime.datetime(2019, 3, 8, 2, 7, 47, 585000)}
2019-03-08 10:16:49,237 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 10:23:27,615 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 10:23:27,622 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 10:23:27,632 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 10:23:27,683 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:23:28,148 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:23:28,154 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:23:28,154 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:26:38,559 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:202.199.159.130:40670
2019-03-08 10:26:38,591 - engine.py[line:256] - INFO: Spider opened
2019-03-08 10:26:38,612 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:26:38,628 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 10:26:38,632 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:26:38,635 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:26:38,638 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:26:38,638 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 10:27:02,786 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:122.114.71.79:808
2019-03-08 10:27:02,901 - engine.py[line:256] - INFO: Spider opened
2019-03-08 10:27:02,901 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:27:03,331 - scraper.py[line:208] - ERROR: Error downloading %(request)s
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
ResponseFailed: [<twisted.python.failure.Failure twisted.web._newclient.ParseError: (u'non-integer status code', '<h1>\xd5\xca\xba\xc5\xc8\xcf\xd6\xa4\xca\xa7\xb0\xdc ...</h1>')>]
2019-03-08 10:27:03,431 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 10:27:03,434 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.web._newclient.ResponseFailed': 3,
 'downloader/request_bytes': 831,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 2, 27, 3, 433000),
 'log_count/DEBUG': 4,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseFailed': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 3, 8, 2, 27, 2, 903000)}
2019-03-08 10:27:03,436 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 10:27:38,612 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:28:38,614 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:29:38,612 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:30:38,612 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:31:38,614 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 10:31:57,680 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 10:31:57,684 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 9,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 5,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 4,
 'downloader/request_bytes': 25153,
 'downloader/request_count': 45,
 'downloader/request_method_count/GET': 45,
 'downloader/response_bytes': 1158355,
 'downloader/response_count': 36,
 'downloader/response_status_count/200': 36,
 'finish_reason': 'cancelled',
 'finish_time': datetime.datetime(2019, 3, 8, 2, 31, 57, 683000),
 'item_scraped_count': 2166,
 'log_count/DEBUG': 2216,
 'log_count/ERROR': 1,
 'log_count/INFO': 24,
 'request_depth_max': 36,
 'response_received_count': 36,
 'retry/count': 9,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 5,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 4,
 'scheduler/dequeued': 45,
 'scheduler/dequeued/memory': 45,
 'scheduler/enqueued': 46,
 'scheduler/enqueued/memory': 46,
 'start_time': datetime.datetime(2019, 3, 8, 2, 26, 38, 614000)}
2019-03-08 10:31:57,684 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 11:15:49,865 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 11:15:49,871 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 11:15:49,882 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 11:15:49,937 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 11:15:50,362 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 11:15:50,369 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 11:15:50,371 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 11:16:22,293 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.247.152.98:53281
2019-03-08 11:16:22,325 - engine.py[line:256] - INFO: Spider opened
2019-03-08 11:16:22,338 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 11:16:22,351 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 11:16:22,355 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 11:16:22,358 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 11:16:22,361 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 11:16:22,361 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 11:16:36,046 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:27.191.234.69:9999
2019-03-08 11:16:36,059 - engine.py[line:256] - INFO: Spider opened
2019-03-08 11:16:36,061 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 11:17:14,229 - scraper.py[line:158] - ERROR: Spider error processing %(request)s (referer: %(referer)s)
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\workspace\pycharm\house_spider\scrapy_spider\spiders\house.py", line 190, in parse
    house_url = self.base_house_url % self.db_building.get("web_build_id")
AttributeError: 'NoneType' object has no attribute 'get'
2019-03-08 11:17:14,273 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 11:17:14,276 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/request_bytes': 1973,
 'downloader/request_count': 5,
 'downloader/request_method_count/GET': 5,
 'downloader/response_bytes': 61009,
 'downloader/response_count': 4,
 'downloader/response_status_count/200': 4,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 3, 17, 14, 274000),
 'item_scraped_count': 5,
 'log_count/DEBUG': 113,
 'log_count/ERROR': 1,
 'log_count/INFO': 8,
 'request_depth_max': 3,
 'response_received_count': 4,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 1,
 'scheduler/dequeued': 5,
 'scheduler/dequeued/memory': 5,
 'scheduler/enqueued': 5,
 'scheduler/enqueued/memory': 5,
 'spider_exceptions/AttributeError': 1,
 'start_time': datetime.datetime(2019, 3, 8, 3, 16, 36, 61000)}
2019-03-08 11:17:14,276 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 11:17:22,338 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 11:17:45,730 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 11:17:45,739 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,
 'downloader/request_bytes': 2050,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 50539,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 8, 3, 17, 45, 733000),
 'item_scraped_count': 100,
 'log_count/DEBUG': 116,
 'log_count/ERROR': 2,
 'log_count/INFO': 20,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2019, 3, 8, 3, 16, 22, 338000)}
2019-03-08 11:17:45,740 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-08 13:09:56,342 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-08 13:09:56,348 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-08 13:09:56,358 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 13:09:56,405 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 13:09:56,835 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 13:09:56,839 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 13:09:56,841 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 13:11:05,013 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-03-08 13:11:05,032 - engine.py[line:256] - INFO: Spider opened
2019-03-08 13:11:05,046 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 13:11:05,062 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-08 13:11:05,065 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 13:11:05,069 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 13:11:05,071 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 13:11:05,071 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-08 13:13:29,463 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.27.170.46:8888
2019-03-08 13:13:30,066 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 13:14:50,194 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:139.129.207.72:808
2019-03-08 13:14:50,198 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 13:15:01,697 - _legacy.py[line:154] - CRITICAL: Unhandled error in Deferred:
2019-03-08 13:15:01,698 - _legacy.py[line:154] - CRITICAL: 
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\crawler.py", line 98, in crawl
    six.reraise(*exc_info)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\crawler.py", line 81, in crawl
    start_requests = iter(self.spider.start_requests())
  File "E:\workspace\pycharm\house_spider\scrapy_spider\spiders\house.py", line 114, in start_requests
    url = self.base_house_url % self.db_building.get("web_build_id")
AttributeError: 'NoneType' object has no attribute 'get'
2019-03-08 13:15:05,048 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-08 13:15:37,158 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-08 13:15:37,164 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 20231,
 'downloader/request_count': 36,
 'downloader/request_method_count/GET': 36,
 'downloader/response_bytes': 1108108,
 'downloader/response_count': 36,
 'downloader/response_status_count/200': 36,
 'finish_reason': 'cancelled',
 'finish_time': datetime.datetime(2019, 3, 8, 5, 15, 37, 162000),
 'item_scraped_count': 2066,
 'log_count/CRITICAL': 2,
 'log_count/DEBUG': 2103,
 'log_count/INFO': 18,
 'request_depth_max': 36,
 'response_received_count': 36,
 'scheduler/dequeued': 36,
 'scheduler/dequeued/memory': 36,
 'scheduler/enqueued': 37,
 'scheduler/enqueued/memory': 37,
 'start_time': datetime.datetime(2019, 3, 8, 5, 11, 5, 47000)}
2019-03-08 13:15:37,164 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
