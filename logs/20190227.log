2019-02-27 09:22:40,917 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 09:22:40,960 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 09:22:40,970 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 09:22:41,326 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:22:42,088 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:22:42,138 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:22:42,171 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:22:45,617 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-02-27 09:22:45,750 - engine.py[line:256] - INFO: Spider opened
2019-02-27 09:22:46,108 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:22:46,108 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 09:22:46,519 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:23:09,509 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:112.12.37.196:53281
2019-02-27 09:24:43,980 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.245.99.52:80
2019-02-27 09:24:43,982 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:24:46,108 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:25:26,403 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.27.24.114:82
2019-02-27 09:25:46,121 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:25:53,387 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:25:53,387 - scraper.py[line:158] - ERROR: Spider error processing %(request)s (referer: %(referer)s)
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\workspace\pycharm\house_spider\scrapy_spider\spiders\house.py", line 170, in parse
    text_url = "http://www.cq315house.com/%s" % response.text.split("self.location=\"/")[1].split("\";}")[0]
IndexError: list index out of range
2019-02-27 09:25:53,467 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-02-27 09:25:53,469 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 2,
 'downloader/request_bytes': 13702,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 2200735,
 'downloader/response_count': 29,
 'downloader/response_status_count/200': 28,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 2, 27, 1, 25, 53, 468000),
 'item_scraped_count': 755,
 'log_count/DEBUG': 787,
 'log_count/ERROR': 3,
 'log_count/INFO': 15,
 'request_depth_max': 27,
 'response_received_count': 29,
 'retry/count': 2,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 2,
 'scheduler/dequeued': 30,
 'scheduler/dequeued/memory': 30,
 'scheduler/enqueued': 30,
 'scheduler/enqueued/memory': 30,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 2, 27, 1, 22, 46, 108000)}
2019-02-27 09:25:53,469 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-02-27 09:27:22,332 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 09:27:22,338 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 09:27:22,348 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 09:27:22,391 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:22,858 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:22,865 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:22,869 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:27,828 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:122.114.71.79:808
2019-02-27 09:27:27,950 - engine.py[line:256] - INFO: Spider opened
2019-02-27 09:27:27,960 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:27:27,960 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 09:27:38,528 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 09:27:38,533 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 09:27:38,543 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 09:27:38,605 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:39,164 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:39,170 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:39,176 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:47,246 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 09:27:47,252 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 09:27:47,262 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 09:27:47,312 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:47,822 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:47,828 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:27:47,832 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:28:08,644 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:58.249.55.222:9797
2019-02-27 09:28:08,799 - engine.py[line:256] - INFO: Spider opened
2019-02-27 09:28:08,815 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:28:08,815 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 09:28:52,605 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:61.128.208.94:3128
2019-02-27 09:28:52,644 - engine.py[line:256] - INFO: Spider opened
2019-02-27 09:28:52,653 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:28:52,654 - middlewares.py[line:109] - INFO: Spider opened: real_estate
2019-02-27 09:29:11,322 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:58.243.50.184:53281
2019-02-27 09:29:11,323 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:29:21,789 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:29:35,821 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:112.91.218.21:9000
2019-02-27 09:29:52,671 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:30:03,859 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-02-27 09:30:04,042 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:30:46,305 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.245.99.52:80
2019-02-27 09:30:46,305 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:30:52,653 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:31:07,141 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:31:07,467 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:31:52,289 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-02-27 09:31:52,653 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:32:07,772 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.61.153.100:53281
2019-02-27 09:32:07,772 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:32:08,815 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:32:20,290 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:32:20,569 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:32:20,668 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.196.204.61:80
2019-02-27 09:32:20,881 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:32:42,750 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.85.196.75:9999
2019-02-27 09:32:58,201 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-02-27 09:32:58,203 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:33:08,815 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:33:20,017 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:61.128.208.94:3128
2019-02-27 09:33:20,424 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-02-27 09:33:20,428 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 2,
 'downloader/request_bytes': 25084,
 'downloader/request_count': 39,
 'downloader/request_method_count/GET': 39,
 'downloader/response_bytes': 1095773,
 'downloader/response_count': 37,
 'downloader/response_status_count/200': 36,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'cancelled',
 'finish_time': datetime.datetime(2019, 2, 27, 1, 33, 20, 428000),
 'item_scraped_count': 2021,
 'log_count/DEBUG': 2061,
 'log_count/INFO': 17,
 'request_depth_max': 36,
 'response_received_count': 37,
 'retry/count': 2,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 2,
 'scheduler/dequeued': 38,
 'scheduler/dequeued/memory': 38,
 'scheduler/enqueued': 39,
 'scheduler/enqueued/memory': 39,
 'start_time': datetime.datetime(2019, 2, 27, 1, 28, 52, 654000)}
2019-02-27 09:33:20,430 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-02-27 09:33:32,404 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:33:32,822 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:34:36,082 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.202.219.81:80
2019-02-27 09:34:36,085 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:34:36,486 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:34:58,539 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.241.219.226:9999
2019-02-27 09:35:41,174 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:113.200.56.13:8010
2019-02-27 09:35:41,174 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:36:05,510 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:36:05,691 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:36:26,828 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.29.9.47:80
2019-02-27 09:36:26,829 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:36:27,299 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-02-27 09:36:27,302 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'downloader/request_bytes': 63724,
 'downloader/request_count': 139,
 'downloader/request_method_count/GET': 139,
 'downloader/response_bytes': 9021881,
 'downloader/response_count': 137,
 'downloader/response_status_count/200': 135,
 'downloader/response_status_count/302': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 2, 27, 1, 36, 27, 301000),
 'item_scraped_count': 3130,
 'log_count/DEBUG': 3270,
 'log_count/ERROR': 12,
 'log_count/INFO': 27,
 'request_depth_max': 134,
 'response_received_count': 136,
 'retry/count': 2,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'scheduler/dequeued': 138,
 'scheduler/dequeued/memory': 138,
 'scheduler/enqueued': 138,
 'scheduler/enqueued/memory': 138,
 'start_time': datetime.datetime(2019, 2, 27, 1, 28, 8, 815000)}
2019-02-27 09:36:27,302 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-02-27 09:36:59,871 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 09:36:59,878 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 09:36:59,888 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 09:36:59,937 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:37:00,413 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:37:00,417 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:37:00,421 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 09:37:05,259 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:112.74.207.50:3128
2019-02-27 09:37:05,275 - engine.py[line:256] - INFO: Spider opened
2019-02-27 09:37:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:37:05,286 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 09:38:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:39:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:39:15,851 - http11.py[line:484] - WARNING: Got data loss in %s. If you want to process broken responses set the setting DOWNLOAD_FAIL_ON_DATALOSS = False -- This message won't be shown in further requests
2019-02-27 09:39:33,667 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-02-27 09:40:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:41:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:42:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:43:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:44:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:45:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:46:05,315 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:46:19,632 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:46:19,723 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.196.204.61:80
2019-02-27 09:46:19,950 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:46:20,088 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:113.200.214.164:9999
2019-02-27 09:47:05,338 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:48:05,362 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:48:14,910 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:48:15,160 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:48:57,486 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.61.153.100:53281
2019-02-27 09:49:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:49:18,250 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:49:18,523 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:49:19,256 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:125.46.0.62:53281
2019-02-27 09:51:07,226 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:123.139.56.238:9999
2019-02-27 09:51:07,229 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:52:01,286 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:52:01,523 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:52:01,862 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.85.196.75:9797
2019-02-27 09:52:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:52:27,841 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:52:28,259 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:53:21,411 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.156.90:53281
2019-02-27 09:53:21,414 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:53:57,517 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:61.145.182.27:53281
2019-02-27 09:54:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:54:32,661 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:27.191.234.69:9999
2019-02-27 09:55:05,315 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:56:05,289 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:57:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 09:57:31,365 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 09:57:31,641 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 10:00:17,565 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:221.178.176.25:3128
2019-02-27 10:00:17,569 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:01:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:01:49,651 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.105.25:53281
2019-02-27 10:02:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:43:23,841 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.29.9.47:80
2019-02-27 10:43:23,842 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:43:45,572 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.85.196.75:9999
2019-02-27 10:44:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:44:12,263 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.108.73:53281
2019-02-27 10:45:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:45:59,119 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:175.102.3.98:8089
2019-02-27 10:46:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:47:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:48:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:49:05,285 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:50:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:51:04,867 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:58.247.127.145:53281
2019-02-27 10:51:05,286 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:52:13,858 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:117.141.155.242:53281
2019-02-27 10:52:13,861 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:53:13,954 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.27.170.46:8888
2019-02-27 10:53:13,960 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:54:39,052 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.90.126.106:7777
2019-02-27 10:54:39,053 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:55:56,065 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:222.221.11.119:3128
2019-02-27 10:55:56,069 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:56:02,479 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:222.16.83.18:80
2019-02-27 10:56:12,733 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.17.139.5:808
2019-02-27 10:56:12,736 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:57:41,890 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.27.24.114:82
2019-02-27 10:57:41,891 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:57:43,444 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:124.152.32.140:53281
2019-02-27 10:58:07,979 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:59.44.247.194:9797
2019-02-27 10:58:07,980 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:59:02,292 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.69.10.62:9090
2019-02-27 10:59:38,177 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:112.12.37.196:53281
2019-02-27 10:59:38,178 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 10:59:59,292 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.14.24.60:9000
2019-02-27 11:01:36,130 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.77.183.125:8080
2019-02-27 11:01:36,131 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 11:01:57,411 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:113.200.56.13:8010
2019-02-27 11:02:03,010 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:222.171.251.43:40149
2019-02-27 11:02:24,733 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:219.245.3.4:3128
2019-02-27 11:02:24,739 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 11:02:45,961 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:180.168.179.193:8080
2019-02-27 11:02:47,198 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:122.114.71.79:808
2019-02-27 11:02:47,434 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.29.230.205:80
2019-02-27 11:03:14,108 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:58.240.220.86:53281
2019-02-27 11:03:14,109 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 11:03:17,457 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:222.74.61.98:53281
2019-02-27 11:04:41,841 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.202.219.81:80
2019-02-27 11:04:41,842 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 11:05:35,098 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.2.203.24:3128
2019-02-27 11:05:35,101 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 11:05:36,325 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.135.183.115:808
2019-02-27 11:05:57,726 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.27.24.114:81
2019-02-27 11:05:57,815 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:113.118.159.127:9000
2019-02-27 11:06:21,536 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.223.187.195:80
2019-02-27 11:06:21,539 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 11:47:18,651 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:220.180.50.14:53281
2019-02-27 11:47:18,653 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 15:15:09,181 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 15:15:09,221 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 15:15:09,233 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 15:15:09,588 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 15:15:10,352 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 15:15:10,401 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 15:15:10,450 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 15:15:15,339 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.196.204.61:80
2019-02-27 15:15:15,464 - engine.py[line:256] - INFO: Spider opened
2019-02-27 15:15:15,786 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 15:15:15,786 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 15:15:16,250 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 15:16:52,362 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:220.180.50.14:53281
2019-02-27 15:16:52,364 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 15:17:15,786 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 15:17:19,805 - scraper.py[line:208] - ERROR: Error downloading %(request)s
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\core\downloader\middleware.py", line 66, in process_exception
    spider=spider)
  File "E:\workspace\pycharm\house_spider\scrapy_spider\middlewares.py", line 125, in process_exception
    print exception.reasons
AttributeError: 'TCPTimedOutError' object has no attribute 'reasons'
2019-02-27 15:17:19,943 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-02-27 15:17:19,946 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/request_bytes': 1482,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 108654,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 2,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 2, 27, 7, 17, 19, 944000),
 'item_scraped_count': 35,
 'log_count/DEBUG': 39,
 'log_count/ERROR': 2,
 'log_count/INFO': 12,
 'request_depth_max': 2,
 'response_received_count': 3,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2019, 2, 27, 7, 15, 15, 787000)}
2019-02-27 15:17:19,947 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-02-27 16:31:20,496 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 16:31:20,500 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 16:31:20,512 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 16:31:20,555 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 16:31:21,029 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 16:31:21,033 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 16:31:21,040 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 16:31:48,667 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.61.153.100:53281
2019-02-27 16:31:48,826 - engine.py[line:256] - INFO: Spider opened
2019-02-27 16:31:48,848 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:31:48,849 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 16:32:46,519 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 16:32:46,806 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 16:32:46,921 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:122.114.71.79:808
2019-02-27 16:32:58,584 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:125.46.0.62:53281
2019-02-27 16:32:58,585 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:33:28,117 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:112.74.207.50:3128
2019-02-27 16:33:32,996 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:221.178.176.25:3128
2019-02-27 16:33:34,753 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.105.25:53281
2019-02-27 16:33:35,200 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.27.24.114:82
2019-02-27 16:34:00,558 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:220.180.50.14:53281
2019-02-27 16:34:00,561 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:34:42,795 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.196.204.61:80
2019-02-27 16:34:43,282 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:27.191.234.69:9999
2019-02-27 16:35:09,301 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.196.170.247:9000
2019-02-27 16:35:09,305 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:36:12,499 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.14.24.60:9000
2019-02-27 16:36:12,500 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:36:55,930 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.202.219.81:80
2019-02-27 16:36:55,933 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:37:17,404 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.245.99.52:80
2019-02-27 16:37:22,888 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.57.108.73:53281
2019-02-27 16:38:31,790 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.77.183.125:8080
2019-02-27 16:38:31,792 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:41:13,973 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:124.152.32.140:53281
2019-02-27 16:41:13,974 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:42:17,288 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.29.230.205:80
2019-02-27 16:42:17,292 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:42:38,483 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.135.183.115:808
2019-02-27 16:42:38,614 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.27.170.46:8888
2019-02-27 16:42:50,878 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:61.145.182.27:53281
2019-02-27 16:42:50,878 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:43:33,171 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:180.168.179.193:8080
2019-02-27 16:45:10,926 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.85.196.75:9797
2019-02-27 16:45:10,927 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:46:17,592 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:113.200.56.13:8010
2019-02-27 16:46:17,595 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:47:00,473 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.85.196.75:9999
2019-02-27 16:47:00,474 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:47:42,931 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.27.24.114:81
2019-02-27 16:47:45,358 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:58.243.50.184:53281
2019-02-27 16:47:45,601 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:113.200.214.164:9999
2019-02-27 16:48:56,170 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.90.126.106:7777
2019-02-27 16:48:56,173 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:50:48,927 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:175.102.3.98:8089
2019-02-27 16:50:48,931 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:51:19,760 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:219.245.3.4:3128
2019-02-27 16:51:28,036 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:222.221.11.119:3128
2019-02-27 16:51:36,684 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:124.205.155.158:9090
2019-02-27 16:53:46,227 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:119.29.9.47:80
2019-02-27 16:53:46,229 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:54:24,789 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:123.139.56.238:9999
2019-02-27 16:54:24,790 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:54:46,563 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:120.237.14.198:53281
2019-02-27 16:55:07,750 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:113.118.159.127:9000
2019-02-27 16:55:07,752 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:55:19,713 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:58.240.220.86:53281
2019-02-27 16:55:20,997 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-02-27 16:55:25,857 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:117.141.155.241:53281
2019-02-27 16:55:49,989 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:58.249.55.222:9797
2019-02-27 16:55:49,990 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 16:55:51,479 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.245.99.52:80
2019-02-27 16:56:37,938 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:112.12.37.196:53281
2019-02-27 18:08:47,997 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 18:08:48,048 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 18:08:48,072 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 18:08:48,438 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:08:49,211 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:08:49,253 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:08:49,286 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:08:53,730 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.14.24.60:9000
2019-02-27 18:08:53,880 - engine.py[line:256] - INFO: Spider opened
2019-02-27 18:08:54,063 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:08:54,065 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 18:08:54,424 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 18:09:15,770 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.27.24.114:81
2019-02-27 18:09:54,063 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:10:13,082 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 18:10:13,084 - scraper.py[line:158] - ERROR: Spider error processing %(request)s (referer: %(referer)s)
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\workspace\pycharm\house_spider\scrapy_spider\spiders\house.py", line 170, in parse
    text_url = "http://www.cq315house.com/%s" % response.text.split("self.location=\"/")[1].split("\";}")[0]
IndexError: list index out of range
2019-02-27 18:10:13,131 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-02-27 18:10:13,138 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 13853,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 3631511,
 'downloader/response_count': 31,
 'downloader/response_status_count/200': 30,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 2, 27, 10, 10, 13, 134000),
 'item_scraped_count': 875,
 'log_count/DEBUG': 907,
 'log_count/ERROR': 3,
 'log_count/INFO': 11,
 'request_depth_max': 29,
 'response_received_count': 31,
 'scheduler/dequeued': 30,
 'scheduler/dequeued/memory': 30,
 'scheduler/enqueued': 30,
 'scheduler/enqueued/memory': 30,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 2, 27, 10, 8, 54, 65000)}
2019-02-27 18:10:13,141 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-02-27 18:10:18,727 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 18:10:18,733 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 18:10:18,743 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 18:10:18,789 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:10:19,230 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:10:19,236 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:10:19,242 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:10:48,809 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:125.46.0.62:53281
2019-02-27 18:10:48,930 - engine.py[line:256] - INFO: Spider opened
2019-02-27 18:10:48,940 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:10:48,940 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 18:11:10,081 - scraper.py[line:208] - ERROR: Error downloading %(request)s
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\twisted\internet\defer.py", line 1418, in _inlineCallbacks
    result = g.send(result)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\core\downloader\middleware.py", line 66, in process_exception
    spider=spider)
  File "E:\workspace\pycharm\house_spider\scrapy_spider\middlewares.py", line 125, in process_exception
    print exception.reasons
AttributeError: 'TCPTimedOutError' object has no attribute 'reasons'
2019-02-27 18:11:10,200 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-02-27 18:11:10,203 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 1,
 'downloader/request_bytes': 667,
 'downloader/request_count': 2,
 'downloader/request_method_count/GET': 2,
 'downloader/response_bytes': 1293,
 'downloader/response_count': 1,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 2, 27, 10, 11, 10, 202000),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 9,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2019, 2, 27, 10, 10, 48, 940000)}
2019-02-27 18:11:10,203 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-02-27 18:11:29,542 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-02-27 18:11:29,548 - log.py[line:149] - INFO: Versions: %(versions)s
2019-02-27 18:11:29,559 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-02-27 18:11:29,601 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:11:30,051 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:11:30,055 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:11:30,061 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-02-27 18:13:10,882 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:113.200.214.164:9999
2019-02-27 18:13:11,323 - engine.py[line:256] - INFO: Spider opened
2019-02-27 18:13:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:13:11,348 - middlewares.py[line:109] - INFO: Spider opened: building
2019-02-27 18:14:00,203 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 18:14:00,464 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 18:14:01,960 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:124.152.32.140:53281
2019-02-27 18:14:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:15:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:16:18,589 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:61.128.208.94:3128
2019-02-27 18:16:18,592 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:17:09,299 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:180.168.179.193:8080
2019-02-27 18:17:10,410 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:121.196.204.61:80
2019-02-27 18:17:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:17:22,661 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 18:18:16,795 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:175.102.3.98:8089
2019-02-27 18:18:16,796 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:19:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:20:11,368 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:21:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:22:11,578 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:23:11,382 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:24:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:25:11,352 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:26:11,359 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:27:11,348 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:28:11,346 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:29:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:30:11,346 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:30:35,207 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:218.17.139.5:808
2019-02-27 18:31:29,642 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:103.27.24.114:82
2019-02-27 18:31:29,644 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:32:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:33:11,736 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:34:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:35:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:36:11,345 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-02-27 18:36:36,220 - house.py[line:160] - ERROR: 没有获取到数据
2019-02-27 18:36:36,220 - scraper.py[line:158] - ERROR: Spider error processing %(request)s (referer: %(referer)s)
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\workspace\pycharm\house_spider\scrapy_spider\spiders\house.py", line 170, in parse
    text_url = "http://www.cq315house.com/%s" % response.text.split("self.location=\"/")[1].split("\";}")[0]
IndexError: list index out of range
2019-02-27 18:36:36,226 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-02-27 18:36:36,227 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 5,
 'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 2,
 'downloader/exception_type_count/twisted.internet.error.TCPTimedOutError': 3,
 'downloader/request_bytes': 100519,
 'downloader/request_count': 220,
 'downloader/request_method_count/GET': 220,
 'downloader/response_bytes': 35802667,
 'downloader/response_count': 215,
 'downloader/response_status_count/200': 214,
 'downloader/response_status_count/404': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 2, 27, 10, 36, 36, 226000),
 'item_scraped_count': 13712,
 'log_count/DEBUG': 13933,
 'log_count/ERROR': 5,
 'log_count/INFO': 39,
 'request_depth_max': 213,
 'response_received_count': 215,
 'retry/count': 5,
 'retry/reason_count/twisted.internet.error.ConnectionRefusedError': 2,
 'retry/reason_count/twisted.internet.error.TCPTimedOutError': 3,
 'scheduler/dequeued': 219,
 'scheduler/dequeued/memory': 219,
 'scheduler/enqueued': 219,
 'scheduler/enqueued/memory': 219,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2019, 2, 27, 10, 13, 11, 347000)}
2019-02-27 18:36:36,227 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
