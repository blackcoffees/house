2019-03-12 11:38:55,065 - log.py[line:146] - INFO: Scrapy %(version)s started (bot: %(bot)s)
2019-03-12 11:38:55,112 - log.py[line:149] - INFO: Versions: %(versions)s
2019-03-12 11:38:55,121 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-12 11:38:55,438 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-12 11:38:56,359 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-12 11:38:56,401 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-12 11:38:56,411 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-12 11:39:04,766 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.85.196.75:9999
2019-03-12 11:39:04,789 - engine.py[line:256] - INFO: Spider opened
2019-03-12 11:39:05,019 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-12 11:39:05,033 - crawler.py[line:38] - INFO: Overridden settings: %(settings)r
2019-03-12 11:39:05,036 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-12 11:39:05,039 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-12 11:39:05,040 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-12 11:39:05,040 - middleware.py[line:53] - INFO: Enabled %(componentname)ss:
%(enabledlist)s
2019-03-12 11:39:05,114 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:183.47.40.35:8088
2019-03-12 11:39:05,148 - engine.py[line:256] - INFO: Spider opened
2019-03-12 11:39:05,148 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-12 11:39:05,575 - house.py[line:163] - ERROR: 没有获取到数据
2019-03-12 11:39:07,108 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:223.85.196.75:9999
2019-03-12 11:40:07,750 - ProxyIPUtil.py[line:156] - INFO: 测试代理IP,切换代理IP:211.162.70.229:3128
2019-03-12 11:40:07,752 - scraper.py[line:158] - ERROR: Spider error processing %(request)s (referer: %(referer)s)
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\utils\defer.py", line 102, in iter_errback
    yield next(it)
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\offsite.py", line 30, in process_spider_output
    for x in result:
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\virtualenv\python27\lib\site-packages\scrapy\spidermiddlewares\depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "E:\workspace\pycharm\house_spider\scrapy_spider\spiders\house.py", line 89, in parse
    page += 1
UnboundLocalError: local variable 'page' referenced before assignment
2019-03-12 11:40:07,798 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-12 11:40:07,799 - logstats.py[line:48] - INFO: Crawled %(pages)d pages (at %(pagerate)d pages/min), scraped %(items)d items (at %(itemrate)d items/min)
2019-03-12 11:40:07,802 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-12 11:40:07,805 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 1214,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 2908,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/302': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 12, 3, 40, 7, 803000),
 'log_count/DEBUG': 8,
 'log_count/ERROR': 2,
 'log_count/INFO': 20,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'spider_exceptions/UnboundLocalError': 1,
 'start_time': datetime.datetime(2019, 3, 12, 3, 39, 5, 20000)}
2019-03-12 11:40:07,805 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
2019-03-12 11:40:08,835 - scraper.py[line:208] - ERROR: Error downloading %(request)s
Traceback (most recent call last):
  File "E:\virtualenv\python27\lib\site-packages\scrapy\core\downloader\middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
ResponseNeverReceived: [<twisted.python.failure.Failure twisted.internet.error.ConnectionLost: Connection to the other side was lost in a non-clean fashion.>]
2019-03-12 11:40:08,951 - engine.py[line:295] - INFO: Closing spider (%(reason)s)
2019-03-12 11:40:08,959 - statscollectors.py[line:47] - INFO: Dumping Scrapy stats:
{'downloader/exception_count': 4,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 4,
 'downloader/request_bytes': 2297,
 'downloader/request_count': 6,
 'downloader/request_method_count/GET': 6,
 'downloader/response_bytes': 498424,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2019, 3, 12, 3, 40, 8, 954000),
 'item_scraped_count': 196,
 'log_count/DEBUG': 207,
 'log_count/ERROR': 3,
 'log_count/INFO': 15,
 'request_depth_max': 2,
 'response_received_count': 2,
 'retry/count': 3,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 3,
 'scheduler/dequeued': 6,
 'scheduler/dequeued/memory': 6,
 'scheduler/enqueued': 6,
 'scheduler/enqueued/memory': 6,
 'start_time': datetime.datetime(2019, 3, 12, 3, 39, 5, 150000)}
2019-03-12 11:40:08,960 - engine.py[line:326] - INFO: Spider closed (%(reason)s)
